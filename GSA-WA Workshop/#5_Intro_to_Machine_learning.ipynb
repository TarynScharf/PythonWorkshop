{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQAzdahVhLsY"
   },
   "source": [
    "# Machine Learning\n",
    "In this module we will see a bit about machine learning and how it can be applied to geosciences.\n",
    "\n",
    "Machine learning is a branch of artificial intelligence (AI) focused on building applications that learn from data and improve their accuracy over time without being programmed to do so. In machine learning, algorithms use statistical techniques to give computers the ability to \"learn\" with incoming data and make decisions based on that data. Machine learning is used in a range of computing tasks where designing and programming explicit, rule-based algorithms is infeasible.\n",
    "\n",
    "Problems that can be tackled by Machine Learning include classification, clustering and regression.\n",
    "- Classification: We know what labels exist and we want to know where our data fits\n",
    "- Clustering: We are not too sure about labels, but we want to see how our data is grouped\n",
    "- Regression: Given the data that we have, what responses should we expect if the inputs were different?\n",
    "\n",
    "\n",
    "In this module, we will explore the application of machine learning algorithms, specifically the Random Forest algorithm. The Random Forest is an ensemble learning method known for its accuracy and ability to handle large datasets with multiple variables. We'll use it to analyze and learn from the geochemical signatures of rocks to predict their geological formation environment.\n",
    "\n",
    "References for this module:\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/index.html)\n",
    "\n",
    "Soares, M. B., et al. (2018). \"Multistage mineralization at the hypozonal São Sebastião gold deposit, Pitangui greenstone belt, Minas Gerais, Brazil.\" Ore Geology Reviews 102: 618-638. https://doi.org/10.1016/j.oregeorev.2018.09.028\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfZZxI2Tg-2y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # For data manipulation\n",
    "import geopandas as gpd\n",
    "import numpy as np  # For numerical operations and random number generation\n",
    "import copy #to create a deepcopy of dataframe\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import re  # For formatting feature names with subscripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPgs1010hdAW"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "url = r\"https://raw.githubusercontent.com/pierosampaio/PythonWorkshop/refs/heads/main/GSA-WA%202025%20Data/MBS2018_Biquinho.csv\"\n",
    "df_Biquinho = pd.read_csv(url, delimiter=',')\n",
    "df_Biquinho = df_Biquinho.reset_index()\n",
    "\n",
    "# Drop any columns that contain only NaN (missing) values\n",
    "# 'axis=1' means we're operating on columns\n",
    "# 'how=\"all\"' means a column is dropped only if *all* its values are NaN\n",
    "df_Biquinho = df_Biquinho.dropna(axis=1, how='all')\n",
    "\n",
    "# Remove any rows that contain at least one NaN value\n",
    "# This helps ensure we are working with complete data for analysis or modeling\n",
    "df_Biquinho = df_Biquinho.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1700532483342,
     "user": {
      "displayName": "Luc-Serge Doucet",
      "userId": "06903758593232990694"
     },
     "user_tz": -480
    },
    "id": "M-bC_8k9lQqZ",
    "outputId": "9a5d60b1-0d3f-45a0-872e-92b46f1ca355",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We explore the dataframe\n",
    "df_Biquinho.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open data for Pimentao deposit\n",
    "url = r\"https://raw.githubusercontent.com/pierosampaio/PythonWorkshop/refs/heads/main/GSA-WA%202025%20Data/MBS2018_Pimentao.csv\"\n",
    "df_Pimentao = pd.read_csv(url, delimiter=',')\n",
    "df_Pimentao = df_Pimentao.reset_index()\n",
    "df_Pimentao = df_Pimentao.dropna(axis=1, how='all')\n",
    "df_Pimentao = df_Pimentao.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Pimentao.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the columns that are common to both dataframes, using the .intersection() method\n",
    "common_columns = df_Biquinho.columns.intersection(df_Pimentao.columns)\n",
    "\n",
    "# Merge the two dataframes while preserving only the columns they have in common\n",
    "# Use the .concat() method to concatenate (stack) the two dataFrames vertically (row-wise)\n",
    "df_combined = pd.concat(\n",
    "    [df_Biquinho[common_columns], df_Pimentao[common_columns]],\n",
    "    ignore_index=True  # Resets the row index in the combined DataFrame to be sequential\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F_cji7-wsb6"
   },
   "source": [
    "# Data sparsity\n",
    "Data sparsity is a common issue that requires careful evaluation, as it can significantly impact machine learning outcomes. Sparse datasets are thinly populated with non-zero values, and are often challenging for algorithms to interpret. Data sparcity can affect model performance and accuracy. It's important to assess the degree of sparsity in the data to develop effective strategies for dealing with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1700536166342,
     "user": {
      "displayName": "Luc-Serge Doucet",
      "userId": "06903758593232990694"
     },
     "user_tz": -480
    },
    "id": "w918WiNVynxL",
    "outputId": "5531a615-8d82-44fc-b745-15218a04995b"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the combined DataFrame for plotting\n",
    "df_combined_plot = copy.deepcopy(df_combined)\n",
    "\n",
    "# Iterate over all unique values in the 'Body' column\n",
    "for body in df_combined_plot['Body'].unique():\n",
    "\n",
    "    # Filter for only  where 'Body' matches the current value\n",
    "    df_body = df_combined_plot[df_combined_plot['Body'] == body]\n",
    "\n",
    "    # Drop the 'index' and 'Sample' columns as they are not needed for this analysis\n",
    "    df_body = df_body.drop(['index', 'Sample'], axis=1)\n",
    "\n",
    "    # Calculate the percentage of non-empty (non-NaN) cells per column\n",
    "    # .notnull() returns True for non-NaN values; .sum() counts the True values per column\n",
    "    # Divide by total number of rows and multiply by 100 to get percentages\n",
    "    non_empty_percentages = (df_body.notnull().sum() / len(df_body)) * 100\n",
    "\n",
    "    # Sort the features by percentage in descending order (most complete first)\n",
    "    sorted_features = non_empty_percentages.sort_values(ascending=False)\n",
    "\n",
    "    # Format feature names for better readability in plots\n",
    "    # Use regular expressions to convert numbers in feature names to subscripts\n",
    "    # For example, 'Pb1' becomes 'Pb$_{1}$'\n",
    "    formatted_features = [\n",
    "        re.sub(r'(\\d+)', r'$_{\\1}$', feature)\n",
    "        for feature in sorted_features.index\n",
    "    ]\n",
    "\n",
    "    # Plot the bar chart \n",
    "    plt.figure(figsize=(10, 4))  # Set figure size\n",
    "    plt.bar(formatted_features, sorted_features.values)  # Create bar plot\n",
    "    plt.xlabel('Features')  # Label for x-axis\n",
    "    plt.ylabel('Percentage of Empty Cells')  # Label for y-axis\n",
    "    plt.title(f'Percentage of Empty Cells for Each Feature – Body: {body}')  # Chart title\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels for readability\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "    plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that there are no missing values in the dataframe. Let's create a sparse dataset to see how it would look in the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the combined DataFrame to avoid altering the original\n",
    "df_sparse = copy.deepcopy(df_combined)\n",
    "\n",
    "# Remove the unnecessary metadata or identifier columns\n",
    "exclude_cols = ['index', 'Body', 'Sample']\n",
    "\n",
    "# Create a list of columns that will be sparsified (all except the excluded ones)\n",
    "target_cols = [col for col in df_sparse.columns if col not in exclude_cols]\n",
    "\n",
    "# Define the range of sparsity to apply\n",
    "# Set the starting sparsity level for the first column (10% values will be missing)\n",
    "start_sparsity = 0.1  # Least sparse\n",
    "\n",
    "# Set the ending sparsity level for the last column (90% values will be missing)\n",
    "end_sparsity = 0.9  # Most sparse\n",
    "\n",
    "# Generate a series of sparsity levels, increasing linearly across the target columns\n",
    "# This creates a gradient from low to high missingness\n",
    "sparsity_levels = np.linspace(start_sparsity, end_sparsity, num=len(target_cols))\n",
    "\n",
    "# Apply the generated sparsity to each target column\n",
    "# Loop over each column and its corresponding sparsity level\n",
    "for col, sparsity in zip(target_cols, sparsity_levels):\n",
    "    \n",
    "    # Generate a random mask where 'True' indicates a value to be set to NaN\n",
    "    # The proportion of 'True' values matches the desired sparsity level\n",
    "    mask = np.random.rand(len(df_sparse)) < sparsity\n",
    "    \n",
    "    # Apply the mask to the column, setting selected values to NaN\n",
    "    df_sparse.loc[mask, col] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all unique values in the 'Body' column of the sparse DataFrame\n",
    "for body in df_sparse['Body'].unique():\n",
    "\n",
    "    # Filter the DataFrame to include only rows belonging to the current Body\n",
    "    # This allows missing data analysis to be performed separately for each geological unit\n",
    "    df_body = df_sparse[df_sparse['Body'] == body]\n",
    "\n",
    "    # Drop the 'index' and 'Sample' columns, as they are not relevant to feature analysis\n",
    "    df_body = df_body.drop(['index', 'Sample'], axis=1)\n",
    "\n",
    "    # Calculate the percentage of non-empty (non-NaN) values for each feature\n",
    "    # .notnull() returns True for non-NaN values; .sum() counts them per column\n",
    "    # Divide by the number of rows and multiply by 100 to get the percentage of filled (non-missing) cells\n",
    "    non_empty_percentages = (df_body.notnull().sum() / len(df_body)) * 100\n",
    "\n",
    "    # Sort the features by percentage of completeness in descending order\n",
    "    sorted_features = non_empty_percentages.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "    # Format feature names for display\n",
    "    # Use regular expressions to convert digits in feature names to subscripts (e.g., 'Pb1' → 'Pb$_{1}$')\n",
    "    formatted_features = [\n",
    "        re.sub(r'(\\d+)', r'$_{\\1}$', feature)\n",
    "        for feature in sorted_features.index\n",
    "    ]\n",
    "\n",
    "    # Plot the bar chart showing missing data percentages-\n",
    "    plt.figure(figsize=(10, 4))  # Set the plot size\n",
    "    plt.bar(formatted_features, sorted_features.values)  # Plot a bar chart of non-empty percentages\n",
    "    plt.xlabel('Features')  # X-axis label\n",
    "    plt.ylabel('Percentage of Empty Cells')  # Y-axis label (note: it's inverse of completeness)\n",
    "    plt.title(f'Percentage of Empty Cells for Each Feature – Body: {body}')  # Chart title indicating the current Body\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout to avoid overlapping elements\n",
    "    plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBPNULFz0OIR"
   },
   "source": [
    "# Sparsity pattern plots\n",
    "\n",
    "Sparsity pattern plots are essential in data analysis for identifying the distribution and prevalence of non-zero values in a dataset. They provide immediate visual feedback on data density and potential patterns, which is critical for understanding the dataset's structure. These plots also highlight areas with missing information, guiding the data cleaning and preprocessing steps. By revealing the extent of sparsity, they help in determining the suitability of various machine learning algorithms, as some algorithms may perform poorly with sparse data. Thus, sparsity pattern plots are a valuable tool for improving data quality and preparing datasets for effective machine learning model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7876,
     "status": "ok",
     "timestamp": 1700536185537,
     "user": {
      "displayName": "Luc-Serge Doucet",
      "userId": "06903758593232990694"
     },
     "user_tz": -480
    },
    "id": "SeoD26cDzb--",
    "outputId": "72f123f8-aa7e-4718-fc23-c5f0f0461e8b"
   },
   "outputs": [],
   "source": [
    "# Extract all unique values from the 'Body' column\n",
    "# Each value represents a distinct geological unit or zone\n",
    "targets = df_sparse[\"Body\"].unique()\n",
    "\n",
    "for target in targets:\n",
    "    title = str(target)  # Convert the body identifier to string for use in plot titles\n",
    "\n",
    "    # Filter for body\n",
    "    df2 = df_sparse[df_sparse[\"Body\"] == target].copy()\n",
    "\n",
    "    # Select only float-type columns for sparsity visualization \n",
    "    # These are typically the numerical geochemical features\n",
    "    df2 = df2.select_dtypes(include=['float'])\n",
    "\n",
    "    # Drop location-related or non-feature columns if present\n",
    "    # 'errors=\"ignore\"' prevents errors if these columns are missing\n",
    "    df2 = df2.drop(columns=['latitude', 'longitude', 'TA'], errors='ignore')\n",
    "\n",
    "    # Prepare the data matrix for plotting\n",
    "    # Convert the DataFrame to a NumPy array\n",
    "    matrix = df2.values\n",
    "\n",
    "    # Count the number of non-empty (non-zero) cells\n",
    "    non_empty_cells = np.count_nonzero(matrix)\n",
    "\n",
    "    # Calculate the number of empty (NaN) or zero cells\n",
    "    empty_cells = matrix.size - non_empty_cells\n",
    "\n",
    "    # Compute the sparsity percentage (proportion of empty cells)\n",
    "    sparsity = round(empty_cells / matrix.size * 100, 1)\n",
    "\n",
    "    # Format tick labels with subscripts for element names (e.g., 'Pb1' → 'Pb$_{1}$')\n",
    "    my_ticks = list(range(len(df2.columns)))  # X-axis tick positions\n",
    "    formatted_ticklabels = [\n",
    "        re.sub(r'(\\d+)', r'$_{\\1}$', col) for col in df2.columns\n",
    "    ]  \n",
    "\n",
    "    # Plot the sparsity pattern using a spy plot\n",
    "    fig = plt.figure(figsize=(8, 2.5))  # Set figure size\n",
    "    ax = fig.add_subplot(111)  # Create subplot\n",
    "    plt.spy(matrix)  # Visualize the sparsity pattern\n",
    "    ax.set_aspect('auto')  # Let the plot auto-adjust its aspect ratio\n",
    "\n",
    "    # Set formatted feature names as x-axis labels\n",
    "    plt.xticks(my_ticks, formatted_ticklabels, rotation=70)\n",
    "    plt.xlabel('Columns')  # Label for features\n",
    "    plt.ylabel('Rows')  # Label for samples\n",
    "\n",
    "    # Set the plot title with detailed sparsity info\n",
    "    ax.set_title(\n",
    "        f\"{title}\\n\\nTotal cells: {matrix.size}\"\n",
    "        f\"\\nNon-empty cells: {non_empty_cells}\"\n",
    "        f\"\\nEmpty cells: {empty_cells}\"\n",
    "        f\"\\n%Sparsity: {sparsity}%\"\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIFQVqpU5Kr5"
   },
   "source": [
    "# Data Imputation\n",
    "Data imputation is a technique used to handle missing values in datasets. It involves substituting the missing or null data points with substituted values, allowing for the continued use of the dataset for analysis without simply discarding incomplete records.\n",
    "\n",
    "Iterative imputation is a specific method of imputation that models each feature with missing values as a function of other features in a round-robin fashion. It's a type of multivariate imputation that estimates the missing values within each feature by treating it as a target variable in a regression, with the other features as predictors. Here's how iterative imputation generally works:\n",
    "\n",
    "1. Initially, all missing values are filled with some initial guess, which could be mean, median, or mode of the column.\n",
    "2. The imputer then models each feature with missing values as a dependent variable, using the other features to predict the missing values.\n",
    "3. This is done in a round-robin or cyclic order, where each feature is imputed in turn and used as a predictor for the next feature to be imputed.\n",
    "4. The process iterates over the dataset multiple times until the change in imputed values converges or until a maximum number of iterations is reached.\n",
    "\n",
    "Using the `IterativeImputer` from scikit-learn, the process can be fine-tuned with parameters like `max_iter` to set the maximum number of imputation iterations, `random_state` for reproducibility of results, and `min_value` or `max_value` to constrain imputed values. After imputation, the data is often more suitable for analysis, as it no longer contains gaps that could skew the results or prevent certain types of computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # Enables experimental IterativeImputer\n",
    "from sklearn.impute import IterativeImputer  # For advanced multivariate imputation\n",
    "from tqdm import tqdm  # For displaying progress bars in loops\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np  # For numerical operations\n",
    "import warnings  # For suppressing warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64316,
     "status": "ok",
     "timestamp": 1700549741909,
     "user": {
      "displayName": "Luc-Serge Doucet",
      "userId": "06903758593232990694"
     },
     "user_tz": -480
    },
    "id": "5YNokFoK5Psb",
    "outputId": "e9bcce77-cda3-45fe-ec10-227055d2d149"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Make a working copy of the combined dataset\n",
    "df_combined_plot2 = copy.deepcopy(df_combined)\n",
    "\n",
    "# Suppress warnings to keep output clean\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize the Iterative Imputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0, min_value=0)\n",
    "\n",
    "# List to collect imputed DataFrames\n",
    "dfs_imputed = []\n",
    "\n",
    "# Loop through each Body group\n",
    "for body_value in tqdm(df_sparse['Body'].unique(), desc='Imputing by Body'):\n",
    "\n",
    "    df = df_sparse[df_sparse['Body'] == body_value].copy()\n",
    "    df_float = df.select_dtypes(include=['float'])\n",
    "\n",
    "    # Log-transform (safe even if data has zeros)\n",
    "    df_float_logged = np.log1p(df_float)\n",
    "\n",
    "    # Identify columns with all NaNs (imputer can’t process these)\n",
    "    columns_with_all_nan = df_float_logged.columns[df_float_logged.isna().all()].tolist()\n",
    "\n",
    "    # Temporarily fill all-NaN columns to allow imputation\n",
    "    if columns_with_all_nan:\n",
    "        df_float_logged[columns_with_all_nan] = df_float_logged[columns_with_all_nan].fillna(-1)\n",
    "\n",
    "    # Standardize (mean=0, std=1) the log-transformed data\n",
    "    scaler = StandardScaler()\n",
    "    df_float_logged_scaled = scaler.fit_transform(df_float_logged)\n",
    "\n",
    "    # Apply imputer to the scaled data\n",
    "    df_imputed_scaled_array = imputer.fit_transform(df_float_logged_scaled)\n",
    "\n",
    "    # Inverse transform the scaling\n",
    "    df_logged_imputed = scaler.inverse_transform(df_imputed_scaled_array)\n",
    "    df_logged_imputed = pd.DataFrame(df_logged_imputed, columns=df_float_logged.columns)\n",
    "\n",
    "    # Restore NaNs for previously all-NaN columns\n",
    "    for col in columns_with_all_nan:\n",
    "        df_logged_imputed[col] = df_logged_imputed[col].replace(-1, np.nan)\n",
    "\n",
    "    # Inverse log transformation\n",
    "    df_float_imputed = np.expm1(df_logged_imputed)\n",
    "\n",
    "    # Recombine with non-float columns\n",
    "    df_non_float = df.drop(columns=df_float.columns).reset_index(drop=True)\n",
    "    df_float_imputed = pd.DataFrame(df_float_imputed, columns=df_float.columns)\n",
    "    df_combined_body = pd.concat([df_non_float, df_float_imputed], axis=1)\n",
    "\n",
    "    dfs_imputed.append(df_combined_body)\n",
    "\n",
    "# Combine all imputed Body-level DataFrames\n",
    "df_imputed = pd.concat(dfs_imputed, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of unique 'Body' values from the imputed DataFrame\n",
    "targets = df_imputed[\"Body\"].unique()\n",
    "\n",
    "# Loop through each body to visualize the sparsity pattern (post-imputation)\n",
    "for target in targets:\n",
    "    title = str(target)  # Convert the body label to string for use in plot titles\n",
    "\n",
    "    # Filter the imputed DataFrame for rows corresponding to the current body\n",
    "    df2 = df_imputed[df_imputed[\"Body\"] == target].copy()\n",
    "\n",
    "    #Select only float-type columns for visualization\n",
    "    df2 = df2.select_dtypes(include=['float'])\n",
    "\n",
    "    # Drop optional location-related or unnecessary columns if they exist\n",
    "    df2 = df2.drop(columns=['latitude', 'longitude', 'TA'], errors='ignore')\n",
    "\n",
    "    #Prepare data for visualization\n",
    "    # Convert the DataFrame to a NumPy matrix for plotting\n",
    "    matrix = df2.values\n",
    "    # Count the number of non-zero (i.e., non-empty or non-zero) cells\n",
    "    non_empty_cells = np.count_nonzero(matrix)\n",
    "\n",
    "    # Calculate the number of empty cells by subtracting from total matrix size\n",
    "    empty_cells = matrix.size - non_empty_cells\n",
    "\n",
    "    # Calculate the sparsity percentage (even though the data is now imputed)\n",
    "    sparsity = round(empty_cells / matrix.size * 100, 1)\n",
    "\n",
    "    # --- Set axis ticks and format column labels with subscripts ---\n",
    "\n",
    "    # X-axis tick positions based on number of columns\n",
    "    my_ticks = list(range(len(df2.columns)))\n",
    "\n",
    "    # Format feature names to show numbers as subscripts (e.g., 'Pb1' → 'Pb$_{1}$')\n",
    "    formatted_ticklabels = [\n",
    "        re.sub(r'(\\d+)', r'$_{\\1}$', col) for col in df2.columns\n",
    "    ]\n",
    "\n",
    "    # Plot the sparsity pattern using a spy plot\n",
    "    fig = plt.figure(figsize=(8, 2.5))  # Set the size of the figure\n",
    "    ax = fig.add_subplot(111)  # Add a subplot to the figure\n",
    "\n",
    "    plt.spy(matrix)  # Create a spy plot showing positions of non-zero elements\n",
    "    ax.set_aspect('auto')  # Allow the aspect ratio to adjust automatically\n",
    "\n",
    "    # Set x-ticks using formatted labels\n",
    "    plt.xticks(my_ticks, formatted_ticklabels, rotation=70)\n",
    "\n",
    "    # Label axes\n",
    "    plt.xlabel('Columns')  # Feature names\n",
    "    plt.ylabel('Rows')  # Sample indices\n",
    "\n",
    "    # Add a detailed title showing basic matrix statistics\n",
    "    ax.set_title(\n",
    "        f\"{title}\\n\\nTotal cells: {matrix.size}\"\n",
    "        f\"\\nNon-empty cells: {non_empty_cells}\"\n",
    "        f\"\\nEmpty cells: {empty_cells}\"\n",
    "        f\"\\n%Sparsity: {sparsity}%\"\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Extract original and imputed values for a specific feature (Au_ppb) ---\n",
    "\n",
    "# Get the 'Au_ppb' column from the original combined DataFrame\n",
    "original = df_combined[\"Au_ppb\"]\n",
    "\n",
    "# Get the corresponding 'Au_ppb' column from the imputed DataFrame\n",
    "imputed = df_imputed[\"Au_ppb\"]\n",
    "\n",
    "# Set visual parameters for the scatter plot \n",
    "symbol_size = 30  # Controls the size of the scatter plot markers\n",
    "\n",
    "# Create a scatter plot of original vs. imputed values\n",
    "plt.figure(figsize=(4, 4))  # Set figure size (width, height in inches)\n",
    "# Red markers with black edges, slightly transparent\n",
    "plt.scatter(\n",
    "    original, imputed,\n",
    "    c='red', s=symbol_size,\n",
    "    alpha=0.6, edgecolor='k',\n",
    "    label='Data points'\n",
    ")\n",
    "\n",
    "# Define axis limits based on the min and max of both datasets\n",
    "lims = [\n",
    "    np.min([original.min(), imputed.min()]),\n",
    "    np.max([original.max(), imputed.max()])\n",
    "]\n",
    "\n",
    "# Plot a dashed line (y = x) to indicate perfect agreement\n",
    "plt.plot(lims, lims, 'k--', linewidth=1, label='1:1 Line')\n",
    "\n",
    "# Add plot labels, legend, and formatting\n",
    "\n",
    "plt.xlabel('Original Au_ppb')  # X-axis label\n",
    "plt.ylabel('Imputed Au_ppb')  # Y-axis label\n",
    "plt.title('Original vs Imputed Au_ppb')  # Title of the plot\n",
    "plt.legend()  # Show legend\n",
    "plt.axis('equal')  # Set the same scale for both axes\n",
    "plt.grid(True)  # Add gridlines\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract original and imputed Au_ppb values ---\n",
    "\n",
    "# Get 'Au_ppb' from the original (sparse) and imputed DataFrames\n",
    "original = df_sparse['Au_ppb'].reset_index(drop=True)\n",
    "imputed = df_imputed['Au_ppb'].reset_index(drop=True)\n",
    "\n",
    "# --- Identify positions that were missing in the original data ---\n",
    "\n",
    "# Create a mask where original values were NaN (i.e., were imputed)\n",
    "missing_mask = original.isna()\n",
    "\n",
    "# Extract only the imputed entries (original was NaN, imputed now filled)\n",
    "original_missing = df_combined.loc[missing_mask, 'Au_ppb']\n",
    "imputed_missing = imputed[missing_mask]\n",
    "\n",
    "# --- Set visual parameters for plotting ---\n",
    "\n",
    "symbol_size = 30  # Size of scatter plot markers\n",
    "\n",
    "# --- Create the plot ---\n",
    "\n",
    "plt.figure(figsize=(4, 4))  # Set figure size\n",
    "\n",
    "# Plot only values that were imputed\n",
    "plt.scatter(\n",
    "    original_missing, imputed_missing,\n",
    "    c='red', s=symbol_size,\n",
    "    alpha=0.6, edgecolor='k',\n",
    "    label='Imputed Points'\n",
    ")\n",
    "\n",
    "# --- Add a 1:1 reference line for visual comparison ---\n",
    "\n",
    "lims = [\n",
    "    np.min([original_missing.min(), imputed_missing.min()]),\n",
    "    np.max([original_missing.max(), imputed_missing.max()])\n",
    "]\n",
    "\n",
    "plt.plot(lims, lims, 'k--', linewidth=1, label='1:1 Line')\n",
    "\n",
    "# --- Add labels, legend, and formatting ---\n",
    "\n",
    "plt.xlabel('True Au_ppb')  # Value in full dataset\n",
    "plt.ylabel('Imputed Au_ppb')  # After imputation\n",
    "plt.title('True vs Imputed Au_ppb (Originally Missing Only)')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the original and imputed V_ppm values from two dataframes\n",
    "original = df_combined[\"V_ppm\"]   # Original (non-imputed) data\n",
    "imputed = df_imputed[\"V_ppm\"]     # Imputed (filled) data\n",
    "\n",
    "# Create a new figure with specified dimensions\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "symbol_size = 30  # You can adjust this for larger/smaller point size\n",
    "\n",
    "# Plot a scatter plot comparing original vs imputed values\n",
    "plt.scatter(\n",
    "    original, imputed,\n",
    "    c='red',                 # Red color for points\n",
    "    s=symbol_size,          # Size of symbols\n",
    "    alpha=0.6,              # Transparency\n",
    "    edgecolor='k',          # Black edge for points\n",
    "    label='Data points'     # Legend label\n",
    ")\n",
    "\n",
    "# Define limits for the 1:1 reference line based on min/max of both datasets\n",
    "lims = [\n",
    "    np.min([original.min(), imputed.min()]),\n",
    "    np.max([original.max(), imputed.max()])\n",
    "]\n",
    "\n",
    "# Plot a dashed 1:1 line (where original = imputed)\n",
    "plt.plot(\n",
    "    lims, lims,\n",
    "    'k--',                 # Black dashed line\n",
    "    linewidth=1,\n",
    "    label='1:1 Line'       # Legend label\n",
    ")\n",
    "\n",
    "# Add axis labels and plot title\n",
    "plt.xlabel('Original V_ppm')\n",
    "plt.ylabel('Imputed V_ppm')\n",
    "plt.title('Original vs Imputed V_ppm')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Set equal scaling on both axes\n",
    "plt.axis('equal')\n",
    "\n",
    "# Add grid for readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Use tight layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Set fixed axis limits\n",
    "plt.xlim(0, 60)\n",
    "plt.ylim(0, 60)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract 'V_ppm' from original and imputed DataFrames ---\n",
    "\n",
    "# Original values before imputation (with NaNs)\n",
    "original = df_sparse['V_ppm'].reset_index(drop=True)\n",
    "\n",
    "# Imputed values (NaNs filled in)\n",
    "imputed = df_imputed['V_ppm'].reset_index(drop=True)\n",
    "\n",
    "# True values from complete dataset (for comparison)\n",
    "true_values = df_combined['V_ppm'].reset_index(drop=True)\n",
    "\n",
    "# --- Identify originally missing values ---\n",
    "\n",
    "missing_mask = original.isna()\n",
    "\n",
    "# Only compare values that were missing and got imputed\n",
    "true_missing = true_values[missing_mask]\n",
    "imputed_missing = imputed[missing_mask]\n",
    "\n",
    "# --- Plotting parameters ---\n",
    "\n",
    "symbol_size = 30  # Size of scatter points\n",
    "\n",
    "# --- Create the scatter plot ---\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(\n",
    "    true_missing, imputed_missing,\n",
    "    c='red',\n",
    "    s=symbol_size,\n",
    "    alpha=0.6,\n",
    "    edgecolor='k',\n",
    "    label='Imputed Points'\n",
    ")\n",
    "\n",
    "# Reference 1:1 line\n",
    "lims = [\n",
    "    np.min([true_missing.min(), imputed_missing.min()]),\n",
    "    np.max([true_missing.max(), imputed_missing.max()])\n",
    "]\n",
    "plt.plot(\n",
    "    lims, lims,\n",
    "    'k--', linewidth=1,\n",
    "    label='1:1 Line'\n",
    ")\n",
    "\n",
    "# --- Plot labels and formatting ---\n",
    "\n",
    "plt.xlabel('True V_ppm')\n",
    "plt.ylabel('Imputed V_ppm')\n",
    "plt.title('True vs Imputed V_ppm (Originally Missing Only)')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, 60)\n",
    "plt.ylim(0, 60)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1700551608789,
     "user": {
      "displayName": "Luc-Serge Doucet",
      "userId": "06903758593232990694"
     },
     "user_tz": -480
    },
    "id": "4EI70CwB-cbX",
    "outputId": "cd8eb893-6bb2-4088-cad2-768b59d14287"
   },
   "outputs": [],
   "source": [
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Columns to exclude from evaluation\n",
    "exclude_cols = ['index', 'Body', 'Sample']\n",
    "\n",
    "# Identify numeric features to evaluate\n",
    "features = [\n",
    "    col for col in df_combined.columns\n",
    "    if col in df_imputed.columns and col not in exclude_cols\n",
    "    and pd.api.types.is_numeric_dtype(df_combined[col])\n",
    "]\n",
    "\n",
    "# Results list\n",
    "results = []\n",
    "\n",
    "# Loop through each numeric feature\n",
    "for feature in features:\n",
    "    # Create a mask where original (sparse) data was missing\n",
    "    missing_mask = df_sparse[feature].isna()\n",
    "\n",
    "    # Skip if no missing values\n",
    "    if not missing_mask.any():\n",
    "        continue\n",
    "\n",
    "    # Extract true and imputed values only for originally missing data\n",
    "    true_values = df_combined.loc[missing_mask, feature]\n",
    "    imputed_values = df_imputed.loc[missing_mask, feature]\n",
    "\n",
    "    # Skip if either is empty\n",
    "    if true_values.empty or imputed_values.empty:\n",
    "        continue\n",
    "\n",
    "    # Calculate correlation and RMSE\n",
    "    corr = true_values.corr(imputed_values)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, imputed_values))\n",
    "\n",
    "    # Append to results\n",
    "    results.append({'Feature': feature, 'Correlation': corr, 'RMSE': rmse})\n",
    "\n",
    "    # Optional: plot\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(true_values, imputed_values, c='red', s=30, alpha=0.6,\n",
    "                edgecolor='k', label='Imputed Points')\n",
    "\n",
    "    lims = [\n",
    "        np.min([true_values.min(), imputed_values.min()]),\n",
    "        np.max([true_values.max(), imputed_values.max()])\n",
    "    ]\n",
    "    plt.plot(lims, lims, 'k--', linewidth=1, label='1:1 Line')\n",
    "\n",
    "    plt.xlabel(f'True {feature}')\n",
    "    plt.ylabel(f'Imputed {feature}')\n",
    "    plt.title(f'{feature} (Originally Missing Only)\\nCorr: {corr:.2f} | RMSE: {rmse:.2f}')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create summary DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Correlation and RMSE Summary (for originally missing values only):\")\n",
    "print(results_df)\n",
    "\n",
    "# Optional: save results to CSV\n",
    "# results_df.to_csv(\"correlation_rmse_summary_missing_only.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest using original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    if 'Body' in df_combined.columns and 'Au_ppb' in df_combined.columns:\n",
    "        df_filtered = df_combined.dropna(subset=['Au_ppb', 'Body']).reset_index(drop=True)\n",
    "\n",
    "        if 'index' in df_filtered.columns:\n",
    "            df_filtered = df_filtered.drop(columns=['index'])\n",
    "\n",
    "        # Split data\n",
    "        train_df, holdout_df = train_test_split(df_filtered, test_size=0.5, random_state=42)\n",
    "\n",
    "        # Prepare test sets per Body\n",
    "        bodies = holdout_df['Body'].unique()\n",
    "        test_sets = {}\n",
    "        for body in bodies:\n",
    "            subset = holdout_df[holdout_df['Body'] == body]\n",
    "            if len(subset) >= 2:\n",
    "                test_sets[body] = subset.sample(frac=0.5, random_state=42)\n",
    "\n",
    "        # Train set\n",
    "        X_train = train_df.drop(columns=['Au_ppb'])\n",
    "        y_train_raw = train_df['Au_ppb']\n",
    "        y_train = np.log1p(y_train_raw)\n",
    "\n",
    "        X_train = pd.get_dummies(X_train)\n",
    "        X_train = X_train.fillna(X_train.mean())\n",
    "\n",
    "        # Train model\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Loop over test bodies\n",
    "        for body_name, test_df in test_sets.items():\n",
    "            X_test = test_df.drop(columns=['Au_ppb'])\n",
    "            y_test_raw = test_df['Au_ppb']\n",
    "            y_test = np.log1p(y_test_raw)\n",
    "\n",
    "            X_test = pd.get_dummies(X_test)\n",
    "\n",
    "            # Align features\n",
    "            X_train_aligned, X_test_aligned = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "            X_test_aligned = X_test_aligned.fillna(X_train.mean())\n",
    "\n",
    "            # Predict\n",
    "            y_pred_log = model.predict(X_test_aligned)\n",
    "\n",
    "            # Back-transform predictions and actuals\n",
    "            y_pred = np.expm1(y_pred_log)\n",
    "            y_actual = np.expm1(y_test)\n",
    "\n",
    "            # Plot: Actual vs Predicted\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.scatter(y_actual, y_pred, alpha=0.6)\n",
    "            lims = [min(y_actual.min(), y_pred.min()), max(y_actual.max(), y_pred.max())]\n",
    "            plt.plot(lims, lims, 'r--')\n",
    "            plt.xlabel(\"Actual Au_ppb\")\n",
    "            plt.ylabel(\"Predicted Au_ppb\")\n",
    "            plt.title(f\"Actual vs Predicted Au_ppb\\nTest: {body_name}\")\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Plot: Top 10 Feature Importances\n",
    "            feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "            top_features = feature_importances.sort_values(ascending=False).head(10)\n",
    "\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            top_features.plot(kind='barh')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.xlabel(\"Importance\")\n",
    "            plt.title(\"Top 10 Feature Importances\\nModel Trained on log(Au_ppb)\")\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"Required columns 'Body' or 'Au_ppb' not found in the dataset.\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"The dataset 'df_combined' is not defined. Please upload it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    if 'Body' in df_combined.columns and 'Au_ppb' in df_combined.columns:\n",
    "        df_filtered = df_combined.dropna(subset=['Au_ppb', 'Body']).reset_index(drop=True)\n",
    "\n",
    "        if 'index' in df_filtered.columns:\n",
    "            df_filtered = df_filtered.drop(columns=['index'])\n",
    "\n",
    "        bodies = df_filtered['Body'].dropna().unique()\n",
    "\n",
    "        if len(bodies) >= 2:\n",
    "            body_name_1 = bodies[0]\n",
    "            body_name_2 = bodies[1]\n",
    "\n",
    "            def train_and_predict(train_body, test_body, train_name, test_name):\n",
    "                train_df = df_filtered[df_filtered['Body'] == train_body]\n",
    "                test_df = df_filtered[df_filtered['Body'] == test_body]\n",
    "\n",
    "                X_train = train_df.drop(columns=['Au_ppb'])\n",
    "                y_train_raw = train_df['Au_ppb']\n",
    "                y_train = np.log1p(y_train_raw)\n",
    "\n",
    "                X_test = test_df.drop(columns=['Au_ppb'])\n",
    "                y_test_raw = test_df['Au_ppb']\n",
    "                y_test = np.log1p(y_test_raw)\n",
    "\n",
    "                X_train = pd.get_dummies(X_train)\n",
    "                X_test = pd.get_dummies(X_test)\n",
    "\n",
    "                X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "                X_train = X_train.fillna(X_train.mean())\n",
    "                X_test = X_test.fillna(X_train.mean())\n",
    "\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred_log = model.predict(X_test)\n",
    "\n",
    "                # Back-transform predictions and true values\n",
    "                y_pred = np.expm1(y_pred_log)\n",
    "                y_actual = np.expm1(y_test)\n",
    "\n",
    "                plt.figure(figsize=(4, 4))\n",
    "                plt.scatter(y_actual, y_pred, alpha=0.6)\n",
    "                lims = [min(y_actual.min(), y_pred.min()), max(y_actual.max(), y_pred.max())]\n",
    "                plt.plot(lims, lims, 'r--')\n",
    "                plt.xlabel(\"Actual Au_ppb\")\n",
    "                plt.ylabel(\"Predicted Au_ppb\")\n",
    "                plt.title(f\"Train: {train_name}, Test: {test_name}\")\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "                top_features = feature_importances.sort_values(ascending=False).head(10)\n",
    "\n",
    "                plt.figure(figsize=(4, 4))\n",
    "                top_features.plot(kind='barh')\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.xlabel(\"Importance\")\n",
    "                plt.title(f\"Top 10 Feature Importances\\nTrain: {train_name}\")\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            train_and_predict(body_name_1, body_name_2, body_name_1, body_name_2)\n",
    "            train_and_predict(body_name_2, body_name_1, body_name_2, body_name_1)\n",
    "\n",
    "        else:\n",
    "            print(\"Not enough unique bodies to perform cross-validation.\")\n",
    "    else:\n",
    "        print(\"Required columns 'Body' or 'Au_ppb' not found in the dataset.\")\n",
    "except NameError:\n",
    "    print(\"The dataset 'df_combined' is not defined. Please upload it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest using Imputed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Check if the df_imputed DataFrame exists and contains the required columns\n",
    "try:\n",
    "    if 'Body' in df_imputed.columns and 'Au_ppb' in df_imputed.columns:\n",
    "        # Remove rows with missing values in 'Body' or 'Au_ppb' and reset the index\n",
    "        df_filtered = df_imputed.dropna(subset=['Au_ppb', 'Body']).reset_index(drop=True)\n",
    "\n",
    "        # Drop 'index' column if it exists (it could be a leftover from previous manipulations)\n",
    "        if 'index' in df_filtered.columns:\n",
    "            df_filtered = df_filtered.drop(columns=['index'])\n",
    "\n",
    "        # Randomly split the filtered data into 50% training and 50% holdout sets\n",
    "        train_df, holdout_df = train_test_split(df_filtered, test_size=0.5, random_state=42)\n",
    "\n",
    "        # For each body in the holdout set, select 50% of its samples to use as test data\n",
    "        bodies = holdout_df['Body'].unique()\n",
    "        test_sets = {}  # Dictionary to store test samples for each body\n",
    "        for body in bodies:\n",
    "            subset = holdout_df[holdout_df['Body'] == body]\n",
    "            if len(subset) >= 2:  # Only include bodies with at least 2 samples\n",
    "                test_sets[body] = subset.sample(frac=0.5, random_state=42)\n",
    "\n",
    "        # Prepare training features and target\n",
    "        X_train = train_df.drop(columns=['Au_ppb'])  # Drop target column from training set\n",
    "        y_train = train_df['Au_ppb']                 # Target variable for training\n",
    "\n",
    "        # One-hot encode categorical features in training data\n",
    "        X_train = pd.get_dummies(X_train)\n",
    "\n",
    "        # Fill missing values with column means in training data\n",
    "        X_train = X_train.fillna(X_train.mean())\n",
    "\n",
    "        # Initialize and train a Random Forest regression model\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Loop through each body-specific test set for evaluation\n",
    "        for body_name, test_df in test_sets.items():\n",
    "            # Prepare test features and target\n",
    "            X_test = test_df.drop(columns=['Au_ppb'])\n",
    "            y_test = test_df['Au_ppb']\n",
    "\n",
    "            # One-hot encode test features\n",
    "            X_test = pd.get_dummies(X_test)\n",
    "\n",
    "            # Align test columns with training columns (adds missing ones with 0)\n",
    "            X_train_aligned, X_test_aligned = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "            # Fill any missing values in the test set with training column means\n",
    "            X_test_aligned = X_test_aligned.fillna(X_train.mean())\n",
    "\n",
    "            # Predict Au_ppb values using the trained model\n",
    "            y_pred = model.predict(X_test_aligned)\n",
    "\n",
    "            # Plot: Actual vs Predicted Au_ppb values\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "            plt.plot(\n",
    "                [y_test.min(), y_test.max()],\n",
    "                [y_test.min(), y_test.max()],\n",
    "                'r--'  # Red dashed 1:1 line\n",
    "            )\n",
    "            plt.xlabel(\"Actual Au_ppb\")\n",
    "            plt.ylabel(\"Predicted Au_ppb\")\n",
    "            plt.title(f\"Actual vs Predicted Au_ppb\\nTest: {body_name}\")\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "            # Get feature importances from the trained model\n",
    "            feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "            top_features = feature_importances.sort_values(ascending=False).head(10)\n",
    "\n",
    "            # Plot: Top 10 most important features\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            top_features.plot(kind='barh')  # Horizontal bar chart\n",
    "            plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "            plt.xlabel(\"Importance\")\n",
    "            plt.title(\"Top 10 Feature Importances\\nModel Trained on 50% Dataset\")\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        # If required columns are not found in df_imputed\n",
    "        print(\"Required columns 'Body' or 'Au_ppb' not found in the dataset.\")\n",
    "\n",
    "# If df_imputed itself is not defined\n",
    "except NameError:\n",
    "    print(\"The dataset 'df_imputed' is not defined. Please upload it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    if 'Body' in df_imputed.columns and 'Au_ppb' in df_imputed.columns:\n",
    "        df_filtered = df_imputed.dropna(subset=['Au_ppb', 'Body']).reset_index(drop=True)\n",
    "\n",
    "        if 'index' in df_filtered.columns:\n",
    "            df_filtered = df_filtered.drop(columns=['index'])\n",
    "\n",
    "        bodies = df_filtered['Body'].dropna().unique()\n",
    "\n",
    "        if len(bodies) >= 2:\n",
    "            body_name_1 = bodies[0]\n",
    "            body_name_2 = bodies[1]\n",
    "\n",
    "            def train_and_predict(train_body, test_body, train_name, test_name):\n",
    "                train_df = df_filtered[df_filtered['Body'] == train_body]\n",
    "                test_df = df_filtered[df_filtered['Body'] == test_body]\n",
    "\n",
    "                X_train = train_df.drop(columns=['Au_ppb'])\n",
    "                y_train_raw = train_df['Au_ppb']\n",
    "                y_train = np.log1p(y_train_raw)\n",
    "\n",
    "                X_test = test_df.drop(columns=['Au_ppb'])\n",
    "                y_test_raw = test_df['Au_ppb']\n",
    "                y_test = np.log1p(y_test_raw)\n",
    "\n",
    "                X_train = pd.get_dummies(X_train)\n",
    "                X_test = pd.get_dummies(X_test)\n",
    "\n",
    "                X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "                X_train = X_train.fillna(X_train.mean())\n",
    "                X_test = X_test.fillna(X_train.mean())\n",
    "\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred_log = model.predict(X_test)\n",
    "\n",
    "                # Back-transform predictions and actuals\n",
    "                y_pred = np.expm1(y_pred_log)\n",
    "                y_actual = np.expm1(y_test)\n",
    "\n",
    "                plt.figure(figsize=(4, 4))\n",
    "                plt.scatter(y_actual, y_pred, alpha=0.6)\n",
    "                lims = [min(y_actual.min(), y_pred.min()), max(y_actual.max(), y_pred.max())]\n",
    "                plt.plot(lims, lims, 'r--')\n",
    "                plt.xlabel(\"Actual Au_ppb\")\n",
    "                plt.ylabel(\"Predicted Au_ppb\")\n",
    "                plt.title(f\"Train: {train_name}, Test: {test_name}\")\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "                top_features = feature_importances.sort_values(ascending=False).head(10)\n",
    "\n",
    "                plt.figure(figsize=(4, 4))\n",
    "                top_features.plot(kind='barh')\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.xlabel(\"Importance\")\n",
    "                plt.title(f\"Top 10 Feature Importances\\nTrain: {train_name}\")\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            train_and_predict(body_name_1, body_name_2, body_name_1, body_name_2)\n",
    "            train_and_predict(body_name_2, body_name_1, body_name_2, body_name_1)\n",
    "\n",
    "        else:\n",
    "            print(\"Not enough unique bodies to perform cross-validation.\")\n",
    "    else:\n",
    "        print(\"Required columns 'Body' or 'Au_ppb' not found in the dataset.\")\n",
    "except NameError:\n",
    "    print(\"The dataset 'df_imputed' is not defined. Please upload it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Get the list of unique body names from the 'Body' column of the DataFrame\n",
    "unique_bodies = df_combined['Body'].unique()\n",
    "\n",
    "# Loop over each body to perform analysis and plotting individually\n",
    "for body in unique_bodies:\n",
    "    # Filter the dataset to only include rows belonging to the current body\n",
    "    subset = df_combined[df_combined['Body'] == body]\n",
    "\n",
    "    # Extract the Bi_ppm values (independent variable, x-axis)\n",
    "    x = subset['Bi_ppm']\n",
    "\n",
    "    # Extract the Au_ppb values (dependent variable, y-axis)\n",
    "    y = subset['Au_ppb']\n",
    "\n",
    "    # Create a mask to exclude rows with missing (NaN) values in either variable\n",
    "    mask = x.notna() & y.notna()\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    # Proceed only if we have at least 2 data points to perform regression\n",
    "    if len(x) >= 2:\n",
    "        # Perform linear regression on the filtered data\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "\n",
    "        # Calculate the regression line (predicted y values)\n",
    "        line = slope * x + intercept\n",
    "\n",
    "        # Calculate R² (coefficient of determination)\n",
    "        r_squared = r_value ** 2\n",
    "\n",
    "        # Create a new figure with fixed size\n",
    "        plt.figure(figsize=(4, 4))\n",
    "\n",
    "        # Scatter plot of actual data points\n",
    "        plt.scatter(x, y, label='Data points', alpha=0.6)\n",
    "\n",
    "        # Plot the regression line\n",
    "        plt.plot(x, line, color='red', label=f'Fit: y = {slope:.2f}x + {intercept:.2f}')\n",
    "\n",
    "        # Add title and axis labels\n",
    "        plt.title(f'Au_ppb vs Bi_ppm for Body: {body}')\n",
    "        plt.xlabel('Bi_ppm')\n",
    "        plt.ylabel('Au_ppb')\n",
    "\n",
    "        # Show background grid\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Display R² value in a text box in the top-left corner of the plot\n",
    "        plt.text(\n",
    "            0.05, 0.95, f'$R^2$ = {r_squared:.2f}',     # Text to display\n",
    "            transform=plt.gca().transAxes,              # Position in axes (not data) coordinates\n",
    "            verticalalignment='top',                    # Align text from top\n",
    "            bbox=dict(                                  # Add a background box to make text readable\n",
    "                facecolor='white',\n",
    "                alpha=0.6,\n",
    "                edgecolor='gray'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Display legend in the bottom-right corner of the plot\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # Adjust layout to avoid clipping of labels\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        # If there are not enough data points to perform regression, print a message\n",
    "        print(f\"Not enough data to plot regression for body: {body}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a concise FAQ covering key questions around machine learning workflows and predictive modeling:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. What is the difference between supervised and unsupervised learning?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Supervised learning** uses labeled data (features + known targets) to train models that predict those targets.\n",
    "* **Unsupervised learning** works with unlabeled data to find structure (e.g., clustering, dimensionality reduction) without explicit targets.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. How should I split my data into training and test sets?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* A common split is **70–80 %** training, **20–30 %** test.\n",
    "* Use **stratification** for classification tasks to preserve class proportions.\n",
    "* If you have time-series data, split by **time** (no random shuffle) to avoid “peeking” into the future.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. When do I need to scale or normalize features?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Tree-based models** (Random Forest, XGBoost) don’t require scaling.\n",
    "* **Distance-based** (KNN), **linear** (Ridge/Lasso), and **gradient-based** (neural nets) models often need features on a **comparable scale** (e.g., StandardScaler or MinMaxScaler).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Should I apply transformations (e.g., log) to features or target?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Features:** Rarely needed for tree models; for linear models, can help linearize relationships or reduce skew.\n",
    "* **Target (`y`):** Use `log1p()` if it’s heavily right-skewed (large dynamic range); then back-transform (`expm1`) for interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. How do I handle missing values?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Simple methods:** Drop rows/columns, replace with mean/median/mode.\n",
    "* **Advanced:** KNN imputation, Iterative Imputer (MICE), or model-based imputation.\n",
    "* **Tip:** For methods sensitive to scale or distribution, consider transforming (log) and standardizing before imputation, then inverse-transform afterward.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What evaluation metrics should I use?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Regression:** RMSE, MAE, R².\n",
    "* **Classification:** Accuracy, precision/recall, F1-score, ROC AUC.\n",
    "* Always evaluate on the **test set** (or via **cross-validation**) to estimate real-world performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What is overfitting, and how can I prevent it?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Overfitting** occurs when a model learns noise in the training data and performs poorly on unseen data.\n",
    "* **Prevention:** Use simpler models, regularization (L1/L2), cross-validation, early stopping (for iterative learners), and more training data.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. How do I choose hyperparameters?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* Perform **grid search** or **random search** over plausible ranges.\n",
    "* For expensive models, use **Bayesian optimization** or **successive halving**.\n",
    "* Always wrap hyperparameter tuning in **cross-validation** to avoid “leaking” test data.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. What is feature engineering, and why is it important?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Feature engineering** is creating, transforming, or selecting inputs that help the model learn better.\n",
    "* Techniques include polynomial features, interactions, domain-specific aggregations, and dimensionality reduction (PCA).\n",
    "\n",
    "---\n",
    "\n",
    "### 10. When should I use cross-validation?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* Use **k-fold CV** (commonly k = 5 or 10) for robust performance estimates, especially with limited data.\n",
    "* For **time-series**, use **time-series CV** that respects temporal order.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. How do I interpret feature importance?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Tree models** provide built-in importance scores (based on split gains or permutation).\n",
    "* **Caution:** Importance can be biased when features are correlated; consider **permutation importance** for more reliable estimates.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. What is ensemble learning?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* Combining multiple models to improve accuracy and robustness.\n",
    "* **Bagging** (e.g., Random Forest), **boosting** (e.g., XGBoost), and **stacking** are common approaches.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. How do I deploy a trained model?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* **Serialize** the model (e.g., with `joblib` or `pickle`).\n",
    "* Expose via an **API** (Flask/FastAPI) or integrate into a larger application.\n",
    "* Monitor in production for **data drift** and **model decay**, and retrain as needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. How can I detect when my model needs retraining?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "* Track performance metrics over time.\n",
    "* Use **statistical tests** or **drift detectors** to compare new data distribution against training data.\n",
    "* Establish thresholds or alerts when metrics degrade beyond acceptable limits.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
